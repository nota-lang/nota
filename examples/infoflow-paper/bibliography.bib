@inproceedings{horwitz1988interprocedural,
  title={Interprocedural slicing using dependence graphs},
  author={Horwitz, Susan and Reps, Thomas and Binkley, David},
  booktitle={Proceedings of the ACM SIGPLAN 1988 conference on Programming Language design and Implementation},
  pages={35--46},
  year={1988}
}

@article{ferrante1987program,
author = {Ferrante, Jeanne and Ottenstein, Karl J. and Warren, Joe D.},
title = {The Program Dependence Graph and Its Use in Optimization},
year = {1987},
issue_date = {July 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {0164-0925},
url = {https://doi.org/10.1145/24039.24041},
doi = {10.1145/24039.24041},
abstract = {In this paper we present an intermediate program representation, called the program dependence graph (PDG), that makes explicit both the data and control dependences for each operation in a program. Data dependences have been used to represent only the relevant data flow relationships of a program. Control dependences are introduced to analogously represent only the essential control flow relationships of a program. Control dependences are derived from the usual control flow graph. Many traditional optimizations operate more efficiently on the PDG. Since dependences in the PDG connect computationally related parts of the program, a single walk of these dependences is sufficient to perform many optimizations. The PDG allows transformations such as vectorization, that previously required special treatment of control dependence, to be performed in a manner that is uniform for both control and data dependences. Program transformations that require interaction of the two dependence types can also be easily handled with our representation. As an example, an incremental approach to modifying data dependences resulting from branch deletion or loop unrolling is introduced. The PDG supports incremental optimization, permitting transformations to be triggered by one another and applied only to affected dependences.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {jul},
pages = {319–349},
numpages = {31}
}



@article{cooper2001simple,
  title={A simple, fast dominance algorithm},
  author={Cooper, Keith D and Harvey, Timothy J and Kennedy, Ken},
  journal={Software Practice \& Experience},
  volume={4},
  number={1-10},
  pages={1--8},
  year={2001}
}

@inproceedings{weiser1984program,
author = {Weiser, Mark},
title = {Program Slicing},
year = {1981},
isbn = {0897911466},
publisher = {IEEE Press},
abstract = {Program slicing is a method used by experienced computer programmers for abstracting from programs. Starting from a subset of a program's behavior, slicing reduces that program to a minimal form which still produces that behavior. The reduced program, called a “slice”, is an independent program guaranteed to faithfully represent the original program within the domain of the specified subset of behavior.Finding a slice is in general unsolvable. A dataflow algorithm is presented for approximating slices when the behavior subset is specified as the values of a set of variables at a statement. Experimental evidence is presented that these slices are used by programmers during debugging. Experience with two automatic slicing tools is summarized. New measures of program complexity are suggested based on the organization of a program's slices.},
booktitle = {Proceedings of the 5th International Conference on Software Engineering},
pages = {439–449},
numpages = {11},
keywords = {Human factors, Software tools, Data flow analysis, Program maintenance, Program metrics, Debugging},
location = {San Diego, California, USA},
series = {ICSE '81}
}


@article{weiser1982programmers,
  title={Programmers use slices when debugging},
  author={Weiser, Mark},
  journal={Communications of the ACM},
  volume={25},
  number={7},
  pages={446--452},
  year={1982},
  publisher={ACM New York, NY, USA}
}

@article{xu2005brief,
  title={A brief survey of program slicing},
  author={Xu, Baowen and Qian, Ju and Zhang, Xiaofang and Wu, Zhongqiang and Chen, Lin},
  journal={ACM SIGSOFT Software Engineering Notes},
  volume={30},
  number={2},
  pages={1--36},
  year={2005},
  publisher={ACM New York, NY, USA}
}

@article{silva2012vocabulary,
  title={A vocabulary of program slicing-based techniques},
  author={Silva, Josep},
  journal={ACM computing surveys (CSUR)},
  volume={44},
  number={3},
  pages={1--41},
  year={2012},
  publisher={ACM New York, NY, USA}
}

@inproceedings{parnin2011automated,
  title={Are automated debugging techniques actually helping programmers?},
  author={Parnin, Chris and Orso, Alessandro},
  booktitle={Proceedings of the 2011 international symposium on software testing and analysis},
  pages={199--209},
  year={2011}
}

@inproceedings{cuoq2012frama,
  title={Frama-c},
  author={Cuoq, Pascal and Kirchner, Florent and Kosmatov, Nikolai and Prevosto, Virgile and Signoles, Julien and Yakobowski, Boris},
  booktitle={International conference on software engineering and formal methods},
  pages={233--247},
  year={2012},
  organization={Springer}
}

@inproceedings{balakrishnan2005codesurfer,
  title={Codesurfer/x86—a platform for analyzing x86 executables},
  author={Balakrishnan, Gogul and Gruian, Radu and Reps, Thomas and Teitelbaum, Tim},
  booktitle={International Conference on Compiler Construction},
  pages={250--254},
  year={2005},
  organization={Springer}
}

@inproceedings{zhao2018parallel,
  title={Parallel sparse flow-sensitive points-to analysis},
  author={Zhao, Jisheng and Burke, Michael G and Sarkar, Vivek},
  booktitle={Proceedings of the 27th International Conference on Compiler Construction},
  pages={59--70},
  year={2018}
}

@inproceedings{might2010resolving,
  title={Resolving and exploiting the k-CFA paradox: illuminating functional vs. object-oriented program analysis},
  author={Might, Matthew and Smaragdakis, Yannis and Van Horn, David},
  booktitle={Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages={305--315},
  year={2010}
}

@inproceedings{clarke1998ownership,
author = {Clarke, David G. and Potter, John M. and Noble, James},
title = {Ownership Types for Flexible Alias Protection},
year = {1998},
isbn = {1581130058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/286936.286947},
doi = {10.1145/286936.286947},
abstract = {Object-oriented programming languages allow inter-object aliasing. Although necessary to construct linked data structures and networks of interacting objects, aliasing is problematic in that an aggregate object's state can change via an alias to one of its components, without the aggregate being aware of any aliasing.Ownership types form a static type system that indicates object ownership. This provides a flexible mechanism to limit the visibility of object references and restrict access paths to objects, thus controlling a system's dynamic topology. The type system is shown to be sound, and the specific aliasing properties that a system's object graph satisfies are formulated and proven invariant for well-typed programs.},
booktitle = {Proceedings of the 13th ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {48–64},
numpages = {17},
keywords = {alias protection, containment, ownership, programming language design, representation exposure, sharing},
location = {Vancouver, British Columbia, Canada},
series = {OOPSLA '98}
}

@inproceedings{grossman2002region,
author = {Grossman, Dan and Morrisett, Greg and Jim, Trevor and Hicks, Michael and Wang, Yanling and Cheney, James},
title = {Region-Based Memory Management in Cyclone},
year = {2002},
isbn = {1581134630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/512529.512563},
doi = {10.1145/512529.512563},
abstract = {Cyclone is a type-safe programming language derived from C. The primary design goal of Cyclone is to let programmers control data representation and memory management without sacrificing type-safety. In this paper, we focus on the region-based memory management of Cyclone and its static typing discipline. The design incorporates several advancements, including support for region subtyping and a coherent integration with stack allocation and a garbage collector. To support separate compilation, Cyclone requires programmers to write some explicit region annotations, but a combination of default annotations, local type inference, and a novel treatment of region effects reduces this burden. As a result, we integrate C idioms in a region-based framework. In our experience, porting legacy C to Cyclone has required altering about 8% of the code; of the changes, only 6% (of the 8%) were region annotations.},
booktitle = {Proceedings of the ACM SIGPLAN 2002 Conference on Programming Language Design and Implementation},
pages = {282–293},
numpages = {12},
location = {Berlin, Germany},
series = {PLDI '02}
}


@inproceedings{agrawal2001evaluating,
  title={Evaluating explicitly context-sensitive program slicing},
  author={Agrawal, Gagan and Guo, Liang},
  booktitle={Proceedings of the 2001 ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools and engineering},
  pages={6--12},
  year={2001}
}

@article{girard1987linear,
  doi = {10.1016/0304-3975(87)90045-4},
  url = {https://doi.org/10.1016/0304-3975(87)90045-4},
  year = {1987},
  publisher = {Elsevier {BV}},
  volume = {50},
  number = {1},
  pages = {1--101},
  author = {Jean-Yves Girard},
  title = {Linear logic},
  journal = {Theoretical Computer Science}
}

@misc{weiss2019oxide,
Author = {Aaron Weiss and Olek Gierczak and Daniel Patterson and Amal Ahmed},
Title = {Oxide: The Essence of Rust},
Year = {2019},
Eprint = {arXiv:1903.00982v3},
}

@inproceedings{rountev1999data,
author = {Rountev, Atanas and Ryder, Barbara G. and Landi, William},
title = {Data-Flow Analysis of Program Fragments},
year = {1999},
isbn = {3540665382},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Traditional interprocedural data-flow analysis is performed on whole programs; however, such whole-program analysis is not feasible for large or incomplete programs. We propose fragment data-flow analysis as an alternative approach which computes data-flow information for a specific program fragment. The analysis is parameterized by the additional information available about the rest of the program. We describe two frameworks for interprocedural flow-sensitive fragment analysis, the relationship between fragment analysis and whole-program analysis, and the requirements ensuring fragment analysis safety and feasibility. We propose an application of fragment analysis as a second analysis phase after an inexpensive flow-insensitive whole-program analysis, in order to obtain better information for important program fragments. We also describe the design of two fragment analyses derived from an already existing whole-program flow- and context-sensitive pointer alias analysis for C programs and present empirical evaluation of their cost and precision. Our experiments show evidence of dramatically better precision obtainable at a practical cost.},
booktitle = {Proceedings of the 7th European Software Engineering Conference Held Jointly with the 7th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {235–252},
numpages = {18},
location = {Toulouse, France},
series = {ESEC/FSE-7}
}


@inproceedings{cousot2002modular,
author = {Cousot, Patrick and Cousot, Radhia},
title = {Modular Static Program Analysis},
year = {2002},
isbn = {3540433694},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The purpose of this paper is to present four basic methods for compositional separate modular static analysis of programs by abstract interpretation: - simplification-based separate analysis; - worst-case separate analysis; - separate analysis with (user-provided) interfaces; - symbolic relational separate analysis; as well as a fifth category which is essentially obtained by composition of the above separate local analyses together with global analysis methods.},
booktitle = {Proceedings of the 11th International Conference on Compiler Construction},
pages = {159–178},
numpages = {20},
series = {CC '02}
}


@inproceedings{gulwani2007computing,
  title={Computing procedure summaries for interprocedural analysis},
  author={Gulwani, Sumit and Tiwari, Ashish},
  booktitle={European Symposium on Programming},
  pages={253--267},
  year={2007},
  organization={Springer}
}


@inproceedings{yorsh2008generating,
author = {Yorsh, Greta and Yahav, Eran and Chandra, Satish},
title = {Generating Precise and Concise Procedure Summaries},
year = {2008},
isbn = {9781595936899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1328438.1328467},
doi = {10.1145/1328438.1328467},
abstract = {We present a framework for generating procedure summaries that are (a) precise - applying the summary in a given context yields the same result as re-analyzing the procedure in that context, and(b) concise - the summary exploits the commonalitiesin the ways the procedure manipulates abstract values, and does not contain superfluous context information.The use of a precise and concise procedure summary inmodular analyses provides a way to capture infinitely many possible contexts in a finite way; in interprocedural analyses, it provides a compact representation of an explicit input-output summary table without loss of precision.We define a class of abstract domains and transformers for which precise and concise summaries can be efficiently generated using our framework. Our framework is rich enough to encode a wide range of problems, including all IFDS and IDE problems. In addition, we show how the framework is instantiated to provide novel solutions to two hard problems: modular linear constant propagation and modular typestate verification, both in the presence of aliasing. We implemented a prototype of our framework that computes summaries for the typestate domain, and report on preliminary experimental results.},
booktitle = {Proceedings of the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {221–234},
numpages = {14},
keywords = {micro-transformers, composition, typestate verification, symbolic summary, relational analysis, summarization, dataflow analysis, aliasing},
location = {San Francisco, California, USA},
series = {POPL '08}
}


@book{sharir1978two,
  title={Two approaches to interprocedural data flow analysis},
  author={Sharir, Micha and Pnueli, Amir and others},
  year={1978},
  publisher={New York University. Courant Institute of Mathematical Sciences}
}

@inproceedings{tang2015summary,
author = {Tang, Hao and Wang, Xiaoyin and Zhang, Lingming and Xie, Bing and Zhang, Lu and Mei, Hong},
title = {Summary-Based Context-Sensitive Data-Dependence Analysis in Presence of Callbacks},
year = {2015},
isbn = {9781450333009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676726.2676997},
doi = {10.1145/2676726.2676997},
abstract = {Building a summary for library code is a common approach to speeding up the analysis of client code. In presence of callbacks, some reachability relationships between library nodes cannot be obtained during library-code summarization. Thus, the library code may have to be analyzed again during the analysis of the client code with the library summary. In this paper, we propose to summarize library code with tree-adjoining-language (TAL) reachability. Compared with the summary built with context-free-language (CFL) reachability, the summary built with TAL reachability further contains conditional reachability relationships. The conditional reachability relationships can lead to much lighter analysis of the library code during the client code analysis with the TAL-reachability-based library summary. We also performed an experimental comparison of context-sensitive data-dependence analysis with the TAL-reachability-based library summary and context-sensitive data-dependence analysis with the CFL-reachability-based library summary using 15 benchmark subjects. Our experimental results demonstrate that the former has an 8X speed-up over the latter on average.},
booktitle = {Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {83–95},
numpages = {13},
keywords = {summary-based analysis, context-sensitive analysis, tree adjoining languages, cfl reachability, tal reachability},
location = {Mumbai, India},
series = {POPL '15}
}


@inproceedings{madhavan2012modular,
  title={Modular heap analysis for higher-order programs},
  author={Madhavan, Ravichandhran and Ramalingam, Ganesan and Vaswani, Kapil},
  booktitle={International Static Analysis Symposium},
  pages={370--387},
  year={2012},
  organization={Springer}
}
 
@inproceedings{wadler1989theorems,
  title={Theorems for free!},
  author={Wadler, Philip},
  booktitle={Proceedings of the fourth international conference on Functional programming languages and computer architecture},
  pages={347--359},
  year={1989}
}

@article{tofte1997region,
  title={Region-based memory management},
  author={Tofte, Mads and Talpin, Jean-Pierre},
  journal={Information and computation},
  volume={132},
  number={2},
  pages={109--176},
  year={1997},
  publisher={Elsevier}
}

@misc{nllrfc,
    author={Niko Matsakis},
    title={Non-lexical lifetimes},
    year={2017},
    url={https://rust-lang.github.io/rfcs/2094-nll.html}
}

@misc{polonius,
  author={Niko Matsakis},
  title={An alias-based formulation of the borrow checker},
  year={2018},
  url={http://smallcultfollowing.com/babysteps/blog/2018/04/27/an-alias-based-formulation-of-the-borrow-checker}
}

@article{jung2017rustbelt,
author = {Jung, Ralf and Jourdan, Jacques-Henri and Krebbers, Robbert and Dreyer, Derek},
title = {RustBelt: Securing the Foundations of the Rust Programming Language},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {POPL},
url = {https://doi.org/10.1145/3158154},
doi = {10.1145/3158154},
abstract = {Rust is a new systems programming language that promises to overcome the seemingly fundamental tradeoff between high-level safety guarantees and low-level control over resource management. Unfortunately, none of Rust's safety claims have been formally proven, and there is good reason to question whether they actually hold. Specifically, Rust employs a strong, ownership-based type system, but then extends the expressive power of this core type system through libraries that internally use unsafe features. In this paper, we give the first formal (and machine-checked) safety proof for a language representing a realistic subset of Rust. Our proof is extensible in the sense that, for each new Rust library that uses unsafe features, we can say what verification condition it must satisfy in order for it to be deemed a safe extension to the language. We have carried out this verification for some of the most important libraries that are used throughout the Rust ecosystem.},
journal = {Proc. ACM Program. Lang.},
month = {dec},
articleno = {66},
numpages = {34},
keywords = {logical relations, separation logic, type systems, concurrency, Rust}
}

@book{appel1997modern,
  title={Modern Compiler Implementation in ML},
  author={Appel, Andrew W},
  year={1997},
  publisher={Cambridge University Press}
}

@misc{mirguide,
    title={The MIR (Mid-level IR) - Guide to Rustc Development},
    year={2021},
    url={https://rustc-dev-guide.rust-lang.org/mir/index.html},
}

@inproceedings{cytron1989efficient,
author = {Cytron, R. and Ferrante, J. and Rosen, B. K. and Wegman, M. N. and Zadeck, F. K.},
title = {An Efficient Method of Computing Static Single Assignment Form},
year = {1989},
isbn = {0897912942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/75277.75280},
doi = {10.1145/75277.75280},
booktitle = {Proceedings of the 16th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {25–35},
numpages = {11},
location = {Austin, Texas, USA},
series = {POPL '89}
}


@misc{cloc,
    title={cloc: Count Lines of Code},
    author={Al Danial},
    year={2021},
    url={https://github.com/AlDanial/cloc}
}

@mastersthesis{llvmslicer,
 author={Marek Chalupa},
 title = {Slicing of LLVM bitcode},
 school = {Masaryk University},
 year = {2016},
} 

@inproceedings{jayaraman2005kaveri,
  title={Kaveri: Delivering the Indus Java program slicer to Eclipse},
  author={Jayaraman, Ganeshan and Ranganath, Venkatesh Prasad and Hatcliff, John},
  booktitle={International Conference on Fundamental Approaches to Software Engineering},
  pages={269--272},
  year={2005},
  organization={Springer}
}


@inproceedings{abadi1999core,
author = {Abadi, Mart\'{\i}n and Banerjee, Anindya and Heintze, Nevin and Riecke, Jon G.},
title = {A Core Calculus of Dependency},
year = {1999},
isbn = {1581130953},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/292540.292555},
doi = {10.1145/292540.292555},
abstract = {Notions of program dependency arise in many settings: security, partial evaluation, program slicing, and call-tracking. We argue that there is a central notion of dependency common to these settings that can be captured within a single calculus, the Dependency Core Calculus (DCC), a small extension of Moggi's computational lambda calculus. To establish this thesis, we translate typed calculi for secure information flow, binding-time analysis, slicing, and call-tracking into DCC. The translations help clarify aspects of the source calculi. We also define a semantic model for DCC and use it to give simple proofs of noninterference results for each case.},
booktitle = {Proceedings of the 26th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {147–160},
numpages = {14},
location = {San Antonio, Texas, USA},
series = {POPL '99}
}


@phdthesis{andersen1994program,
  title={Program analysis and specialization for the C programming language},
  author={Andersen, Lars Ole},
  year={1994},
  school={Citeseer}
}

@inproceedings{steensgaard1996points,
  title={Points-to analysis in almost linear time},
  author={Steensgaard, Bjarne},
  booktitle={Proceedings of the 23rd ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
  pages={32--41},
  year={1996}
} 

@article{pottier2003information,
author = {Pottier, Fran\c{c}ois and Simonet, Vincent},
title = {Information Flow Inference for ML},
year = {2003},
issue_date = {January 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/596980.596983},
doi = {10.1145/596980.596983},
abstract = {This paper presents a type-based information flow analysis for a call-by-value λ-calculus equipped with references, exceptions and let-polymorphism, which we refer to as ML. The type system is constraint-based and has decidable type inference. Its noninterference proof is reasonably light-weight, thanks to the use of a number of orthogonal techniques. First, a syntactic segregation between values and expressions allows a lighter formulation of the type system. Second, noninterference is reduced to subject reduction for a nonstandard language extension. Lastly, a semi-syntactic approach to type soundness allows dealing with constraint-based polymorphism separately.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {jan},
pages = {117–158},
numpages = {42},
keywords = {Constraint-based analysis, non-interference}
}

@inproceedings{campbell2018cognitive,
  title={Cognitive complexity: An overview and evaluation},
  author={Campbell, G Ann},
  booktitle={Proceedings of the 2018 international conference on technical debt},
  pages={57--58},
  year={2018}
}

@article{smaragdakis2015pointer,
  title={Pointer analysis},
  author={Smaragdakis, Yannis and Balatsouras, George},
  journal={Foundations and Trends in Programming Languages},
  volume={2},
  number={1},
  pages={1--69},
  year={2015},
  publisher={Now Publishers Inc. Hanover, MA, USA}
}

@article{astrauskas2019leveraging,
author = {Astrauskas, Vytautas and M\"{u}ller, Peter and Poli, Federico and Summers, Alexander J.},
title = {Leveraging Rust Types for Modular Specification and Verification},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360573},
doi = {10.1145/3360573},
abstract = {Rust's type system ensures memory safety: well-typed Rust programs are guaranteed to not exhibit problems such as dangling pointers, data races, and unexpected side effects through aliased references. Ensuring correctness properties beyond memory safety, for instance, the guaranteed absence of assertion failures or more-general functional correctness, requires static program verification. For traditional system programming languages, formal verification is notoriously difficult and requires complex specifications and logics to reason about pointers, aliasing, and side effects on mutable state. This complexity is a major obstacle to the more-widespread verification of system software. In this paper, we present a novel verification technique that leverages Rust's type system to greatly simplify the specification and verification of system software written in Rust. We analyse information from the Rust compiler and synthesise a corresponding core proof for the program in a flavour of separation logic tailored to automation. To verify correctness properties beyond memory safety, users can annotate Rust programs with specifications at the abstraction level of Rust expressions; our technique weaves them into the core proof to verify modularly whether these specifications hold. Crucially, our proofs are constructed and checked automatically without exposing the underlying formal logic, allowing users to work exclusively at the level of abstraction of the programming language. As such, our work enables a new kind of verification tool, with the potential to impact a wide audience and allow the Rust community to benefit from state-of-the-art verification techniques. We have implemented our techniques for a subset of Rust; our evaluation on several thousand functions from widely-used Rust crates demonstrates its effectiveness.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {147},
numpages = {30},
keywords = {type systems, heap-manipulating programs, concurrency, Rust}
}

@article{jung2020stacked,
author = {Jung, Ralf and Dang, Hoang-Hai and Kang, Jeehoon and Dreyer, Derek},
title = {Stacked Borrows: An Aliasing Model for Rust},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {POPL},
url = {https://doi.org/10.1145/3371109},
doi = {10.1145/3371109},
abstract = {Type systems are useful not just for the safety guarantees they provide, but also for helping compilers generate more efficient code by simplifying important program analyses. In Rust, the type system imposes a strict discipline on pointer aliasing, and it is an express goal of the Rust compiler developers to make use of that alias information for the purpose of program optimizations that reorder memory accesses. The problem is that Rust also supports unsafe code, and programmers can write unsafe code that bypasses the usual compiler checks to violate the aliasing discipline. To strike a balance between optimizations and unsafe code, the language needs to provide a set of rules such that unsafe code authors can be sure, if they are following these rules, that the compiler will preserve the semantics of their code despite all the optimizations it is doing. In this work, we propose Stacked Borrows, an operational semantics for memory accesses in Rust. Stacked Borrows defines an aliasing discipline and declares programs violating it to have undefined behavior, meaning the compiler does not have to consider such programs when performing optimizations. We give formal proofs (mechanized in Coq) showing that this rules out enough programs to enable optimizations that reorder memory accesses around unknown code and function calls, based solely on intraprocedural reasoning. We also implemented this operational model in an interpreter for Rust and ran large parts of the Rust standard library test suite in the interpreter to validate that the model permits enough real-world unsafe Rust code.},
journal = {Proc. ACM Program. Lang.},
month = {dec},
articleno = {41},
numpages = {32},
keywords = {program transformation, operational semantics, Rust, alias analysis}
}



@article{astrauskas2020programmers,
author = {Astrauskas, Vytautas and Matheja, Christoph and Poli, Federico and M\"{u}ller, Peter and Summers, Alexander J.},
title = {How Do Programmers Use Unsafe Rust?},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428204},
doi = {10.1145/3428204},
abstract = {Rust’s ownership type system enforces a strict discipline on how memory locations are accessed and shared. This discipline allows the compiler to statically prevent memory errors, data races, inadvertent side effects through aliasing, and other errors that frequently occur in conventional imperative programs. However, the restrictions imposed by Rust’s type system make it difficult or impossible to implement certain designs, such as data structures that require aliasing (e.g. doubly-linked lists and shared caches). To work around this limitation, Rust allows code blocks to be declared as unsafe and thereby exempted from certain restrictions of the type system, for instance, to manipulate C-style raw pointers. Ensuring the safety of unsafe code is the responsibility of the programmer. However, an important assumption of the Rust language, which we dub the Rust hypothesis, is that programmers use Rust by following three main principles: use unsafe code sparingly, make it easy to review, and hide it behind a safe abstraction such that client code can be written in safe Rust. Understanding how Rust programmers use unsafe code and, in particular, whether the Rust hypothesis holds is essential for Rust developers and testers, language and library designers, as well as tool developers. This paper studies empirically how unsafe code is used in practice by analysing a large corpus of Rust projects to assess the validity of the Rust hypothesis and to classify the purpose of unsafe code. We identify queries that can be answered by automatically inspecting the program’s source code, its intermediate representation MIR, as well as type information provided by the Rust compiler; we complement the results by manual code inspection. Our study supports the Rust hypothesis partially: While most unsafe code is simple and well-encapsulated, unsafe features are used extensively, especially for interoperability with other languages.},
journal = {Proc. ACM Program. Lang.},
month = {nov},
articleno = {136},
numpages = {27},
keywords = {Rust hypothesis, Rust, empirical study, unsafe code}
}

@inproceedings{dillig2011precise,
author = {Dillig, Isil and Dillig, Thomas and Aiken, Alex and Sagiv, Mooly},
title = {Precise and Compact Modular Procedure Summaries for Heap Manipulating Programs},
year = {2011},
isbn = {9781450306638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993498.1993565},
doi = {10.1145/1993498.1993565},
abstract = {We present a strictly bottom-up, summary-based, and precise heap analysis targeted for program verification that performs strong updates to heap locations at call sites. We first present a theory of heap decompositions that forms the basis of our approach; we then describe a full analysis algorithm that is fully symbolic and efficient. We demonstrate the precision and scalability of our approach for verification of real C and C++ programs.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {567–577},
numpages = {11},
keywords = {summary-based analysis, pointer analysis},
location = {San Jose, California, USA},
series = {PLDI '11}
}

@ARTICLE{sabelfeld2003language,
  author={Sabelfeld, A. and Myers, A.C.},
  journal={IEEE Journal on Selected Areas in Communications}, 
  title={Language-based information-flow security}, 
  year={2003},
  volume={21},
  number={1},
  pages={5-19},
  doi={10.1109/JSAC.2002.806121},
  url={https://doi.org/10.1109/JSAC.2002.806121}
}

@incollection{abadi1999protection,
  title={Protection in programming-language translations},
  author={Abadi, Mart{\'\i}n},
  booktitle={Secure Internet programming},
  pages={19--34},
  year={1999},
  publisher={Springer}
}

@inproceedings{myers1999jflow,
author = {Myers, Andrew C.},
title = {JFlow: Practical Mostly-Static Information Flow Control},
year = {1999},
isbn = {1581130953},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/292540.292561},
doi = {10.1145/292540.292561},
abstract = {A promising technique for protecting privacy and integrity of sensitive data is to statically check information flow within programs that manipulate the data. While previous work has proposed programming language extensions to allow this static checking, the resulting languages are too restrictive for practical use and have not been implemented. In this paper, we describe the new language JFlow, an extension to the Java language that adds statically-checked information flow annotations. JFlow provides several new features that make information flow checking more flexible and convenient than in previous models: a decentralized label model, label polymorphism, run-time label checking, and automatic label inference. JFlow also supports many language features that have never been integrated successfully with static information flow control, including objects, subclassing, dynamic type tests, access control, and exceptions. This paper defines the JFlow language and presents formal rules that are used to check JFlow programs for correctness. Because most checking is static, there is little code space, data space, or run-time overhead in the JFlow implementation.},
booktitle = {Proceedings of the 26th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {228–241},
numpages = {14},
location = {San Antonio, Texas, USA},
series = {POPL '99}
}


@inproceedings{goguen1982security,
  doi = {10.1109/sp.1982.10014},
  url = {https://doi.org/10.1109/sp.1982.10014},
  year = {1982},
  month = apr,
  publisher = {{IEEE}},
  author = {J. A. Goguen and J. Meseguer},
  title = {Security Policies and Security Models},
  booktitle = {1982 {IEEE} Symposium on Security and Privacy}
}

@inproceedings{stefan2011flexible,
author = {Stefan, Deian and Russo, Alejandro and Mitchell, John C. and Mazi\`{e}res, David},
title = {Flexible Dynamic Information Flow Control in Haskell},
year = {2011},
isbn = {9781450308601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2034675.2034688},
doi = {10.1145/2034675.2034688},
abstract = {We describe a new, dynamic, floating-label approach to language-based information flow control, and present an implementation in Haskell. A labeled IO monad, LIO, keeps track of a current label and permits restricted access to IO functionality, while ensuring that the current label exceeds the labels of all data observed and restricts what can be modified. Unlike other language-based work, LIO also bounds the current label with a current clearance that provides a form of discretionary access control. In addition, programs may encapsulate and pass around the results of computations with different labels. We give precise semantics and prove confidentiality and integrity properties of the system.},
booktitle = {Proceedings of the 4th ACM Symposium on Haskell},
pages = {95–106},
numpages = {12},
keywords = {library, monad, information flow control},
location = {Tokyo, Japan},
series = {Haskell '11}
}


@inproceedings{austin2009efficient,
author = {Austin, Thomas H. and Flanagan, Cormac},
title = {Efficient Purely-Dynamic Information Flow Analysis},
year = {2009},
isbn = {9781605586458},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1554339.1554353},
doi = {10.1145/1554339.1554353},
abstract = {We present a novel approach for efficiently tracking information flow in a dynamically-typed language such as JavaScript. Our approach is purely dynamic, and it detects problems with implicit paths via a dynamic check that avoids the need for an approximate static analyses while still guaranteeing non-interference. We incorporate this check into an efficient evaluation strategy based on sparse information labeling that leaves information flow labels implicit whenever possible, and introduces explicit labels only for values that migrate between security domains. We present experimental results showing that, on a range of small benchmark programs, sparse labeling provides a substantial (30%--50%) speed-up over universal labeling.},
booktitle = {Proceedings of the ACM SIGPLAN Fourth Workshop on Programming Languages and Analysis for Security},
pages = {113–124},
numpages = {12},
keywords = {information flow control, dynamic analysis},
location = {Dublin, Ireland},
series = {PLAS '09}
}

@inproceedings{gordon2015information,
  title={Information flow analysis of Android applications in DroidSafe},
  author={Gordon, Michael I and Kim, Deokhwan and Perkins, Jeff H and Gilham, Limei and Nguyen, Nguyen and Rinard, Martin C},
  booktitle={NDSS},
  volume={15},
  number={201},
  pages={110},
  year={2015}
}

@inproceedings{shapiro1997effects,
author = {Shapiro, Marc and Horwitz, Susan},
title = {The Effects of the Precision of Pointer Analysis},
year = {1997},
isbn = {3540634681},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the 4th International Symposium on Static Analysis},
pages = {16–34},
numpages = {19},
series = {SAS '97}
}

@article{denning1976lattice,
author = {Denning, Dorothy E.},
title = {A Lattice Model of Secure Information Flow},
year = {1976},
issue_date = {May 1976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/360051.360056},
doi = {10.1145/360051.360056},
abstract = {This paper investigates mechanisms that guarantee secure information flow in a computer system. These mechanisms are examined within a mathematical framework suitable for formulating the requirements of secure information flow among security classes. The central component of the model is a lattice structure derived from the security classes and justified by the semantics of information flow. The lattice properties permit concise formulations of the security requirements of different existing systems and facilitate the construction of mechanisms that enforce security. The model provides a unifying view of all systems that restrict information flow, enables a classification of them according to security objectives, and suggests some new approaches. It also leads to the construction of automatic program certification mechanisms for verifying the secure flow of information through a program.},
journal = {Commun. ACM},
month = {may},
pages = {236–243},
numpages = {8},
keywords = {program certification, lattice, information flow, security, security class, protection}
}

@incollection{tang1994separate,
  doi = {10.1007/3-540-57887-0_98},
  url = {https://doi.org/10.1007/3-540-57887-0_98},
  year = {1994},
  publisher = {Springer Berlin Heidelberg},
  pages = {224--243},
  author = {Yan Mei Tang and Pierre Jouvelot},
  title = {Separate abstract interpretation for control-flow analysis},
  booktitle = {Lecture Notes in Computer Science}
}

@INPROCEEDINGS{li2006encoding,
  author={Peng Li and Zdancewic, S.},
  booktitle={19th IEEE Computer Security Foundations Workshop (CSFW'06)}, 
  title={Encoding information flow in Haskell}, 
  year={2006},
  volume={},
  number={},
  pages={12 pp.-16},
  doi={10.1109/CSFW.2006.13}
}

@inproceedings{russo2008library,
author = {Russo, Alejandro and Claessen, Koen and Hughes, John},
title = {A Library for Light-Weight Information-Flow Security in Haskell},
year = {2008},
isbn = {9781605580647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1411286.1411289},
doi = {10.1145/1411286.1411289},
abstract = {Protecting confidentiality of data has become increasingly important for computing systems. Information-flow techniques have been developed over the years to achieve that purpose, leading to special-purpose languages that guarantee information-flow security in programs. However, rather than producing a new language from scratch, information-flow security can also be provided as a library. This has been done previously in Haskell using the arrow framework. In this paper, we show that arrows are not necessary to design such libraries and that a less general notion, namely monads, is sufficient to achieve the same goals. We present a monadic library to provide information-flow security for Haskell programs. The library introduces mechanisms to protect confidentiality of data for pure computations, that we then easily, and modularly, extend to include dealing with side-effects. We also present combinators to dynamically enforce different declassification policies when release of information is required in a controlled manner. It is possible to enforce policies related to what, by whom, and when information is released or a combination of them. The well-known concept of monads together with the light-weight characteristic of our approach makes the library suitable to build applications where confidentiality of data is an issue.},
booktitle = {Proceedings of the First ACM SIGPLAN Symposium on Haskell},
pages = {13–24},
numpages = {12},
keywords = {monad, library, information-flow, declassification},
location = {Victoria, BC, Canada},
series = {Haskell '08}
}



@inproceedings{buiras2015hlio,
author = {Buiras, Pablo and Vytiniotis, Dimitrios and Russo, Alejandro},
title = {HLIO: Mixing Static and Dynamic Typing for Information-Flow Control in Haskell},
year = {2015},
isbn = {9781450336697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2784731.2784758},
doi = {10.1145/2784731.2784758},
abstract = { Information-Flow Control (IFC) is a well-established approach for allowing untrusted code to manipulate sensitive data without disclosing it. IFC is typically enforced via type systems and static analyses or via dynamic execution monitors. The LIO Haskell library, originating in operating systems research, implements a purely dynamic monitor of the sensitivity level of a computation, particularly suitable when data sensitivity levels are only known at runtime. In this paper, we show how to give programmers the flexibility of deferring IFC checks to runtime (as in LIO), while also providing static guarantees---and the absence of runtime checks---for parts of their programs that can be statically verified (unlike LIO). We present the design and implementation of our approach, HLIO (Hybrid LIO), as an embedding in Haskell that uses a novel technique for deferring IFC checks based on singleton types and constraint polymorphism. We formalize HLIO, prove non-interference, and show how interesting IFC examples can be programmed. Although our motivation is IFC, our technique for deferring constraints goes well beyond and offers a methodology for programmer-controlled hybrid type checking in Haskell. },
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Functional Programming},
pages = {289–301},
numpages = {13},
keywords = {hybrid typing, gradual typing, Information-flow control, singleton types, constraint kinds, data kinds, dynamic typing},
location = {Vancouver, BC, Canada},
series = {ICFP 2015}
}


@inproceedings{balasubramanian2017system,
author = {Balasubramanian, Abhiram and Baranowski, Marek S. and Burtsev, Anton and Panda, Aurojit and Rakamari\'{c}, Zvonimir and Ryzhyk, Leonid},
title = {System Programming in Rust: Beyond Safety},
year = {2017},
isbn = {9781450350686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102980.3103006},
doi = {10.1145/3102980.3103006},
abstract = {Rust is a new system programming language that offers a practical and safe alternative to C. Rust is unique in that it enforces safety without runtime overhead, most importantly, without the overhead of garbage collection. While zero-cost safety is remarkable on its own, we argue that the superpowers of Rust go beyond safety. In particular, Rust's linear type system enables capabilities that cannot be implemented efficiently in traditional languages, both safe and unsafe, and that dramatically improve security and reliability of system software. We show three examples of such capabilities: zero-copy software fault isolation, efficient static information flow analysis, and automatic checkpointing. While these capabilities have been in the spotlight of systems research for a long time, their practical use is hindered by high cost and complexity. We argue that with the adoption of Rust these mechanisms will become commoditized.},
booktitle = {Proceedings of the 16th Workshop on Hot Topics in Operating Systems},
pages = {156–161},
numpages = {6},
location = {Whistler, BC, Canada},
series = {HotOS '17}
}

@inproceedings{rakamaric2014smack,
  title = {SMACK: Decoupling Source Language Details from Verifier Implementations},
  author = {Zvonimir Rakamaric and Michael Emmi},
  booktitle = {Proceedings of the 26th International Conference on Computer Aided Verification (CAV)},
  series = {Lecture Notes in Computer Science},
  volume = {8559},
  publisher = {Springer},
  editor = {Armin Biere and Roderick Bloem},
  pages = {106--113},
  doi = {10.1007/978-3-319-08867-9_7},
    url = {https://doi.org/10.1007/978-3-319-08867-9_7},
year = {2014}
}

@inproceedings{ko2004designing,
author = {Ko, Amy J. and Myers, Brad A.},
title = {Designing the Whyline: A Debugging Interface for Asking Questions about Program Behavior},
year = {2004},
isbn = {1581137028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/985692.985712},
doi = {10.1145/985692.985712},
abstract = {Debugging is still among the most common and costly of programming activities. One reason is that current debugging tools do not directly support the inquisitive nature of the activity. Interrogative Debugging is a new debugging paradigm in which programmers can ask why did and even why didn't questions directly about their program's runtime failures. The Whyline is a prototype Interrogative Debugging interface for the Alice programming environment that visualizes answers in terms of runtime events directly relevant to a programmer's question. Comparisons of identical debugging scenarios from user tests with and without the Whyline showed that the Whyline reduced debugging time by nearly a factor of 8, and helped programmers complete 40% more tasks.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {151–158},
numpages = {8},
keywords = {Alice, program slicing, debugging},
location = {Vienna, Austria},
series = {CHI '04}
}

@inproceedings{head2018interactive,
author = {Head, Andrew and Glassman, Elena L. and Hartmann, Bj\"{o}rn and Hearst, Marti A.},
title = {Interactive Extraction of Examples from Existing Code},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173659},
abstract = {Programmers frequently learn from examples produced and shared by other programmers. However, it can be challenging and time-consuming to produce concise, working code examples. We conducted a formative study where 12 participants made examples based on their own code. This revealed a key hurdle: making meaningful simplifications without introducing errors. Based on this insight, we designed a mixed-initiative tool, CodeScoop, to help programmers extract executable, simplified code from existing code. CodeScoop enables programmers to "scoop" out a relevant subset of code. Techniques include selectively including control structures and recording an execution trace that allows authors to substitute literal values for code and variables. In a controlled study with 19 participants, CodeScoop helped programmers extract executable code examples with the intended behavior more easily than with a standard code editor.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12}
}

@inproceedings{wen2018context,
author = {Wen, Ming and Chen, Junjie and Wu, Rongxin and Hao, Dan and Cheung, Shing-Chi},
title = {Context-Aware Patch Generation for Better Automated Program Repair},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180233},
doi = {10.1145/3180155.3180233},
abstract = {The effectiveness of search-based automated program repair is limited in the number of correct patches that can be successfully generated. There are two causes of such limitation. First, the search space does not contain the correct patch. Second, the search space is huge and therefore the correct patch cannot be generated (i.e., correct patches are either generated after incorrect plausible ones or not generated within the time budget).To increase the likelihood of including the correct patches in the search space, we propose to work at a fine granularity in terms of AST nodes. This, however, will further enlarge the search space, increasing the challenge to find the correct patches. We address the challenge by devising a strategy to prioritize the candidate patches based on their likelihood of being correct. Specifically, we study the use of AST nodes' context information to estimate the likelihood.In this paper, we propose CapGen, a context-aware patch generation technique. The novelty which allows CapGen to produce more correct patches lies in three aspects: (1) The fine-granularity design enables it to find more correct fixing ingredients; (2) The context-aware prioritization of mutation operators enables it to constrain the search space; (3) Three context-aware models enable it to rank correct patches at high positions before incorrect plausible ones. We evaluate CapGen on Defects4J and compare it with the state-of-the-art program repair techniques. Our evaluation shows that CapGen outperforms and complements existing techniques. CapGen achieves a high precision of 84.00% and can prioritize the correct patches before 98.78% of the incorrect plausible ones.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1–11},
numpages = {11},
keywords = {automated program repair, context-aware, patch prioritization},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}



@inproceedings{evans2020rust,
author = {Evans, Ana Nora and Campbell, Bradford and Soffa, Mary Lou},
title = {Is Rust Used Safely by Software Developers?},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380413},
doi = {10.1145/3377811.3380413},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {246–257},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@mastersthesis{njor2021static,
  title={Static Taint Analysis in Rust},
  author={Njor, Emil J{\o}rgensen and G{\'u}stafsson, Hilmar},
  year={2021},
  school = {Aalborg University}
}

